{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from autofeat import AutoFeatRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"diabetes\", \"boston\", \"concrete\", \"airfoil\", \"wine_quality\"]\n",
    "\n",
    "# same interface for loading all datasets - adapt the datapath\n",
    "# to where you've downloaded (and renamed) the datasets\n",
    "def load_regression_dataset(name, datapath=\"../datasets/regression/\"):\n",
    "    # load one of the datasets as X and y (and possibly units)\n",
    "    units = {}\n",
    "    if name == \"boston\":\n",
    "        # sklearn boston housing dataset\n",
    "        X, y = load_boston(True)\n",
    "\n",
    "    elif name == \"diabetes\":\n",
    "        # sklearn diabetes dataset\n",
    "        X, y = load_diabetes(True)\n",
    "\n",
    "    elif name == \"concrete\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Concrete+Compressive+Strength\n",
    "        # Cement (component 1) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Blast Furnace Slag (component 2) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Fly Ash (component 3) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Water (component 4) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Superplasticizer (component 5) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Coarse Aggregate (component 6) -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Fine Aggregate (component 7)    -- quantitative -- kg in a m3 mixture -- Input Variable\n",
    "        # Age -- quantitative -- Day (1~365) -- Input Variable\n",
    "        # Concrete compressive strength -- quantitative -- MPa -- Output Variable\n",
    "        df = pd.read_csv(os.path.join(datapath, \"concrete.csv\"))\n",
    "        X = df.iloc[:, :8].to_numpy()\n",
    "        y = df.iloc[:, 8].to_numpy()\n",
    "\n",
    "    elif name == \"forest_fires\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Forest+Fires\n",
    "        # 1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "        # 2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "        # 3. month - month of the year: 'jan' to 'dec'\n",
    "        # 4. day - day of the week: 'mon' to 'sun'\n",
    "        # 5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "        # 6. DMC - DMC index from the FWI system: 1.1 to 291.3\n",
    "        # 7. DC - DC index from the FWI system: 7.9 to 860.6\n",
    "        # 8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "        # 9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "        # 10. RH - relative humidity in %: 15.0 to 100\n",
    "        # 11. wind - wind speed in km/h: 0.40 to 9.40\n",
    "        # 12. rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "        # 13. area - the burned area of the forest (in ha): 0.00 to 1090.84\n",
    "        # (this output variable is very skewed towards 0.0, thus it may make sense to model with the logarithm transform).\n",
    "        # --> first 4 are ignored\n",
    "        df = pd.read_csv(os.path.join(datapath, \"forest_fires.csv\"))\n",
    "        X = df.iloc[:, 4:12].to_numpy()\n",
    "        y = df.iloc[:, 12].to_numpy()\n",
    "        # perform transformation as they suggested\n",
    "        y = np.log(y + 1)\n",
    "\n",
    "    elif name == \"wine_quality\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "        # Input variables (based on physicochemical tests):\n",
    "        # 1 - fixed acidity\n",
    "        # 2 - volatile acidity\n",
    "        # 3 - citric acid\n",
    "        # 4 - residual sugar\n",
    "        # 5 - chlorides\n",
    "        # 6 - free sulfur dioxide\n",
    "        # 7 - total sulfur dioxide\n",
    "        # 8 - density\n",
    "        # 9 - pH\n",
    "        # 10 - sulphates\n",
    "        # 11 - alcohol\n",
    "        # Output variable (based on sensory data):\n",
    "        # 12 - quality (score between 0 and 10)\n",
    "        df_red = pd.read_csv(os.path.join(datapath, \"winequality-red.csv\"), sep=\";\")\n",
    "        df_white = pd.read_csv(os.path.join(datapath, \"winequality-white.csv\"), sep=\";\")\n",
    "        # add additional categorical feature for red or white\n",
    "        X = np.hstack([np.vstack([df_red.iloc[:, :-1].to_numpy(), df_white.iloc[:, :-1].to_numpy()]), np.array([[1]*len(df_red) + [0]*len(df_white)]).T])\n",
    "        y = np.hstack([df_red[\"quality\"].to_numpy(), df_white[\"quality\"].to_numpy()])\n",
    "\n",
    "    elif name == \"airfoil\":\n",
    "        # https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise\n",
    "        # This problem has the following inputs:\n",
    "        # 1. Frequency, in Hertz.\n",
    "        # 2. Angle of attack, in degrees.\n",
    "        # 3. Chord length, in meters.\n",
    "        # 4. Free-stream velocity, in meters per second.\n",
    "        # 5. Suction side displacement thickness, in meters.\n",
    "        # The only output is:\n",
    "        # 6. Scaled sound pressure level, in decibels.\n",
    "        units = {\"x001\": \"Hz\", \"x003\": \"m\", \"x004\": \"m/sec\", \"x005\": \"m\"}\n",
    "        df = pd.read_csv(os.path.join(datapath, \"airfoil_self_noise.tsv\"), header=None, names=[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"y\"], sep=\"\\t\")\n",
    "        X = df.iloc[:, :5].to_numpy()\n",
    "        y = df[\"y\"].to_numpy()\n",
    "\n",
    "    else:\n",
    "        raise RuntimeError(\"Unknown dataset %r\" % name)\n",
    "    return np.array(X, dtype=float), np.array(y, dtype=float), units\n",
    "\n",
    "def test_model(dataset, model, param_grid):\n",
    "    # load data\n",
    "    X, y, _ = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    if model.__class__.__name__ == \"SVR\":\n",
    "        sscaler = StandardScaler()\n",
    "        X_train = sscaler.fit_transform(X_train)\n",
    "        X_test = sscaler.transform(X_test)\n",
    "    # train model on train split incl cross-validation for parameter selection\n",
    "    gsmodel = GridSearchCV(model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test)))\n",
    "    return gsmodel.best_estimator_\n",
    "\n",
    "def test_autofeat(dataset, feateng_steps=2):\n",
    "    # load data\n",
    "    X, y, units = load_regression_dataset(dataset)\n",
    "    # split in training and test parts\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "    # run autofeat\n",
    "    afreg = AutoFeatRegressor(verbose=1, feateng_steps=feateng_steps, units=units)\n",
    "    # fit autofeat on less data, otherwise ridge reg model with xval will overfit on new features\n",
    "    X_train_tr = afreg.fit_transform(X_train, y_train)\n",
    "    X_test_tr = afreg.transform(X_test)\n",
    "    print(\"autofeat new features:\", len(afreg.new_feat_cols_))\n",
    "    print(\"autofeat MSE on training data:\", mean_squared_error(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat MSE on test data:\", mean_squared_error(y_test, afreg.predict(X_test_tr)))\n",
    "    print(\"autofeat R^2 on training data:\", r2_score(y_train, afreg.predict(X_train_tr)))\n",
    "    print(\"autofeat R^2 on test data:\", r2_score(y_test, afreg.predict(X_test_tr)))\n",
    "    # train rreg on transformed train split incl cross-validation for parameter selection\n",
    "    print(\"# Ridge Regression\")\n",
    "    rreg = Ridge()\n",
    "    param_grid = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000.]}\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        gsmodel = GridSearchCV(rreg, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "        gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"# Random Forest\")\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    param_grid = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    gsmodel = GridSearchCV(rforest, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    gsmodel.fit(X_train_tr, y_train)\n",
    "    print(\"best params:\", gsmodel.best_params_)\n",
    "    print(\"best score:\", gsmodel.best_score_)\n",
    "    print(\"MSE on training data:\", mean_squared_error(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"MSE on test data:\", mean_squared_error(y_test, gsmodel.predict(X_test_tr)))\n",
    "    print(\"R^2 on training data:\", r2_score(y_train, gsmodel.predict(X_train_tr)))\n",
    "    print(\"R^2 on test data:\", r2_score(y_test, gsmodel.predict(X_test_tr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "(442, 10)\n",
      "#### boston\n",
      "(506, 13)\n",
      "#### concrete\n",
      "(1030, 8)\n",
      "#### airfoil\n",
      "(1503, 5)\n",
      "#### wine_quality\n",
      "(6497, 12)\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    X, y, _ = load_regression_dataset(dsname)\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'alpha': 0.01}\n",
      "best score: -3043.1448766877706\n",
      "MSE on training data: 2817.5756461735427\n",
      "MSE on test data: 3119.632550355442\n",
      "R^2 on training data: 0.541317737800587\n",
      "R^2 on test data: 0.38300930348673157\n",
      "#### boston\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -25.427148426837697\n",
      "MSE on training data: 22.4278718761592\n",
      "MSE on test data: 20.55805030529219\n",
      "R^2 on training data: 0.7361592384229154\n",
      "R^2 on test data: 0.7484031841564716\n",
      "#### concrete\n",
      "best params: {'alpha': 10000.0}\n",
      "best score: -110.34480414200303\n",
      "MSE on training data: 107.00865107837936\n",
      "MSE on test data: 110.56229503996859\n",
      "R^2 on training data: 0.6245955930727385\n",
      "R^2 on test data: 0.5643057266127827\n",
      "#### airfoil\n",
      "best params: {'alpha': 0.001}\n",
      "best score: -22.960476312553066\n",
      "MSE on training data: 22.6317043193984\n",
      "MSE on test data: 24.732769352718226\n",
      "R^2 on training data: 0.5173357362628234\n",
      "R^2 on test data: 0.5076580301932745\n",
      "#### wine_quality\n",
      "best params: {'alpha': 0.0001}\n",
      "best score: -0.5401265191014878\n",
      "MSE on training data: 0.5348196387905403\n",
      "MSE on test data: 0.5434554548609962\n",
      "R^2 on training data: 0.29251728914027875\n",
      "R^2 on test data: 0.3100144852264417\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    rreg = Ridge()\n",
    "    params = {\"alpha\": [0.00001, 0.0001, 0.001, 0.01, 0.1, 1., 2.5, 5., 10., 25., 50., 100., 250., 500., 1000., 2500., 5000., 10000., 25000., 50000., 100000.]}\n",
    "    rreg = test_model(dsname, rreg, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'C': 10.0}\n",
      "best score: -3057.7070015538065\n",
      "MSE on training data: 2577.0841482085507\n",
      "MSE on test data: 3437.6800004513143\n",
      "R^2 on training data: 0.5804681274187382\n",
      "R^2 on test data: 0.32010692168648935\n",
      "#### boston\n",
      "best params: {'C': 100.0}\n",
      "best score: -13.598043246310898\n",
      "MSE on training data: 3.4469342608444284\n",
      "MSE on test data: 9.636188588435925\n",
      "R^2 on training data: 0.9594503764998732\n",
      "R^2 on test data: 0.8820688572255264\n",
      "#### concrete\n",
      "best params: {'C': 100.0}\n",
      "best score: -37.08377959823637\n",
      "MSE on training data: 18.997096173790347\n",
      "MSE on test data: 30.152373071635388\n",
      "R^2 on training data: 0.9333549806432162\n",
      "R^2 on test data: 0.8811781514521082\n",
      "#### airfoil\n",
      "best params: {'C': 250.0}\n",
      "best score: -7.094189398840056\n",
      "MSE on training data: 5.457762290823167\n",
      "MSE on test data: 7.477589784074904\n",
      "R^2 on training data: 0.8836028086716045\n",
      "R^2 on test data: 0.8511476320667879\n",
      "#### wine_quality\n",
      "best params: {'C': 10.0}\n",
      "best score: -0.4640006945371349\n",
      "MSE on training data: 0.32390678929749517\n",
      "MSE on test data: 0.4638085434435221\n",
      "R^2 on training data: 0.5715219922060317\n",
      "R^2 on test data: 0.4111363245289218\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    svr = SVR(gamma=\"scale\")\n",
    "    params = {\"C\": [1., 10., 25., 50., 100., 250.]}\n",
    "    svr = test_model(dsname, svr, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3336.6571277553485\n",
      "MSE on training data: 2472.319475907154\n",
      "MSE on test data: 3268.607555103584\n",
      "R^2 on training data: 0.5975231076301993\n",
      "R^2 on test data: 0.35354551553768243\n",
      "#### boston\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.462810946604932\n",
      "MSE on training data: 1.4186988960396048\n",
      "MSE on test data: 10.583239343137262\n",
      "R^2 on training data: 0.9833104719321342\n",
      "R^2 on test data: 0.8704785093673091\n",
      "#### concrete\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -28.701648337050347\n",
      "MSE on training data: 4.169688233206215\n",
      "MSE on test data: 27.527437198114896\n",
      "R^2 on training data: 0.9853720299949222\n",
      "R^2 on test data: 0.8915222703733743\n",
      "#### airfoil\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.770457737476125\n",
      "MSE on training data: 0.4389576916890236\n",
      "MSE on test data: 3.316904702700349\n",
      "R^2 on training data: 0.9906383899294207\n",
      "R^2 on test data: 0.9339721576787678\n",
      "#### wine_quality\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.3930715087362109\n",
      "MSE on training data: 0.05242462959399654\n",
      "MSE on test data: 0.3478176153846154\n",
      "R^2 on training data: 0.9306504167557255\n",
      "R^2 on test data: 0.558401495004132\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    rforest = RandomForestRegressor(n_estimators=100, random_state=13)\n",
    "    params = {\"min_samples_leaf\": [0.0001, 0.001, 0.01, 0.05, 0.1, 0.2]}\n",
    "    rforest = test_model(dsname, rforest, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 70 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 45 transformed features from 10 original features - done.\n",
      "[feateng] Generated altogether 45 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 36 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 8 features after 5 feature selection runs\n",
      "[featsel] 8 features after correlation filtering\n",
      "[featsel] 7 features after noise filtering\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "[AutoFeat] Final dataframe with 12 feature columns (2 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "148.26123404591166\n",
      "-33297.585283 * x008**3\n",
      "2759.036216 * x009**2\n",
      "699.105321 * x008\n",
      "509.425262 * x002\n",
      "350.396359 * x003\n",
      "-272.048144 * x006\n",
      "-233.701750 * x001\n",
      "[AutoFeat] Final score: 0.5528\n",
      "[AutoFeat] Computing 2 new features.\n",
      "[AutoFeat]     2/    2 new features ...done.\n",
      "autofeat new features: 2\n",
      "autofeat MSE on training data: 2747.305320489234\n",
      "autofeat MSE on test data: 3034.113158470636\n",
      "autofeat R^2 on training data: 0.5527572716403053\n",
      "autofeat R^2 on test data: 0.39992304839504966\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -3017.584829689823\n",
      "MSE on training data: 2700.793266039515\n",
      "MSE on test data: 3068.9204297817596\n",
      "R^2 on training data: 0.5603291195811095\n",
      "R^2 on test data: 0.39303898040841134\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3345.207020503157\n",
      "MSE on training data: 2458.150109241621\n",
      "MSE on test data: 3257.334505175377\n",
      "R^2 on training data: 0.5998297847073192\n",
      "R^2 on test data: 0.3557750623880771\n",
      "#### boston\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 91 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 13 original features - done.\n",
      "[feateng] Generated altogether 60 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 22 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 16 features after 5 feature selection runs\n",
      "[featsel] 15 features after correlation filtering\n",
      "[featsel] 14 features after noise filtering\n",
      "[AutoFeat] Computing 6 new features.\n",
      "[AutoFeat]     6/    6 new features ...done.\n",
      "[AutoFeat] Final dataframe with 19 feature columns (6 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "28.098005470706482\n",
      "38.001468 * 1/x012\n",
      "-18.570908 * x004\n",
      "18.502018 * 1/x007\n",
      "5.312438 * 1/x002\n",
      "-4.732514 * 1/x008\n",
      "1.916310 * x003\n",
      "0.951613 * x005\n",
      "-0.439132 * x010\n",
      "-0.364139 * x012\n",
      "-0.139691 * x000\n",
      "-0.004439 * x009\n",
      "0.004379 * x011\n",
      "-0.002540 * x007**3\n",
      "0.002466 * exp(x005)\n",
      "[AutoFeat] Final score: 0.8247\n",
      "[AutoFeat] Computing 6 new features.\n",
      "[AutoFeat]     6/    6 new features ...done.\n",
      "autofeat new features: 6\n",
      "autofeat MSE on training data: 14.898417572390404\n",
      "autofeat MSE on test data: 15.541921099450105\n",
      "autofeat R^2 on training data: 0.8247354960694532\n",
      "autofeat R^2 on test data: 0.8097923780395468\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -17.175220809220974\n",
      "MSE on training data: 13.933824414410099\n",
      "MSE on test data: 16.155065181948398\n",
      "R^2 on training data: 0.8360829388771723\n",
      "R^2 on test data: 0.8022885001659646\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.332717875092584\n",
      "MSE on training data: 1.418291952970299\n",
      "MSE on test data: 11.27794937254902\n",
      "R^2 on training data: 0.9833152591972798\n",
      "R^2 on test data: 0.86197639809027\n",
      "#### concrete\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 56 features.\n",
      "[AutoFeat] With 824 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Generated altogether 34 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 12 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 14 features after 5 feature selection runs\n",
      "[featsel] 13 features after correlation filtering\n",
      "[featsel] 13 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 13 feature columns (5 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-75.24419845949086\n",
      "9.730223 * log(x007)\n",
      "0.579779 * x004\n",
      "0.133681 * x000\n",
      "0.133117 * x001\n",
      "-0.130289 * x003\n",
      "0.097250 * x002\n",
      "0.031676 * x006\n",
      "-0.029365 * x004**2\n",
      "0.029358 * x005\n",
      "-0.022288 * x007\n",
      "[AutoFeat] Final score: 0.8469\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 43.628335878848745\n",
      "autofeat MSE on test data: 46.11691964514932\n",
      "autofeat R^2 on training data: 0.8469444349520285\n",
      "autofeat R^2 on test data: 0.8182664552288247\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -45.75863079774509\n",
      "MSE on training data: 43.622990663445485\n",
      "MSE on test data: 46.23267097653529\n",
      "R^2 on training data: 0.8469631868697298\n",
      "R^2 on test data: 0.8178103124524507\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -28.546330863841312\n",
      "MSE on training data: 4.179885374785849\n",
      "MSE on test data: 27.748865460693214\n",
      "R^2 on training data: 0.9853362567013757\n",
      "R^2 on test data: 0.8906496851404377\n",
      "#### airfoil\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 35 features.\n",
      "[AutoFeat] With 1202 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Generated altogether 21 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 12 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 10 features after 5 feature selection runs\n",
      "[featsel] 10 features after correlation filtering\n",
      "[featsel] 10 features after noise filtering\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "[AutoFeat] Final dataframe with 11 feature columns (6 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "165.4304438269818\n",
      "-39557.243906 * x004**3\n",
      "-2405.824505 * 1/x000\n",
      "-61.784324 * x004\n",
      "-23.840033 * x002\n",
      "-5.042803 * log(x000)\n",
      "-0.134906 * x001\n",
      "0.092407 * x003\n",
      "0.003667 * 1/x004\n",
      "-0.000332 * x000\n",
      "-0.000283 * x001**3\n",
      "[AutoFeat] Final score: 0.5689\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] Computing 5 new features.\n",
      "[AutoFeat]     5/    5 new features ...done.\n",
      "autofeat new features: 5\n",
      "autofeat MSE on training data: 20.21356351138238\n",
      "autofeat MSE on test data: 22.09563974555901\n",
      "autofeat R^2 on training data: 0.5689072015065376\n",
      "autofeat R^2 on test data: 0.560153954402494\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -21.20934670160553\n",
      "MSE on training data: 20.420429186903718\n",
      "MSE on test data: 22.579034186981836\n",
      "R^2 on training data: 0.5644953963875383\n",
      "R^2 on test data: 0.550531280609291\n",
      "# Random Forest\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.906333436423625\n",
      "MSE on training data: 0.4506220271923523\n",
      "MSE on test data: 3.345056032207664\n",
      "R^2 on training data: 0.9903896257255304\n",
      "R^2 on test data: 0.9334117642660995\n",
      "#### wine_quality\n",
      "[AutoFeat] The 1 step feature engineering process could generate up to 84 features.\n",
      "[AutoFeat] With 5197 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 59 transformed features from 12 original features - done.\n",
      "[feateng] Generated altogether 59 new features in 1 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 21 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 24 features after 5 feature selection runs\n",
      "[featsel] 23 features after correlation filtering\n",
      "[featsel] 21 features after noise filtering\n",
      "[AutoFeat] Computing 11 new features.\n",
      "[AutoFeat]    11/   11 new features ...done.\n",
      "[AutoFeat] Final dataframe with 23 feature columns (11 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "115.31028665006441\n",
      "-115.190139 * x007\n",
      "4.510914 * 1/x006\n",
      "-1.211251 * 1/x005\n",
      "0.952697 * x009\n",
      "0.429343 * x008\n",
      "-0.410946 * 1/x003\n",
      "-0.396685 * x001**3\n",
      "-0.393387 * x001\n",
      "0.365975 * x011\n",
      "-0.312603 * x002**3\n",
      "0.196338 * x010\n",
      "0.194578 * x002\n",
      "-0.166206 * x009**3\n",
      "0.091961 * 1/x001\n",
      "0.082164 * x000\n",
      "0.026870 * x003\n",
      "0.015530 * x005\n",
      "0.000987 * x003**2\n",
      "-0.000171 * x005**2\n",
      "[AutoFeat] Final score: 0.3197\n",
      "[AutoFeat] Computing 11 new features.\n",
      "[AutoFeat]    11/   11 new features ...done.\n",
      "autofeat new features: 11\n",
      "autofeat MSE on training data: 0.514285745085418\n",
      "autofeat MSE on test data: 0.5190591958674391\n",
      "autofeat R^2 on training data: 0.319680418033335\n",
      "autofeat R^2 on test data: 0.34098862518520634\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 10000.0}\n",
      "best score: -0.6332731822387769\n",
      "MSE on training data: 0.5970225204223607\n",
      "MSE on test data: 0.6121215236867059\n",
      "R^2 on training data: 0.21023260823423218\n",
      "R^2 on test data: 0.2228342160389657\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.3935680086806841\n",
      "MSE on training data: 0.05247375408889744\n",
      "MSE on test data: 0.3478410769230769\n",
      "R^2 on training data: 0.9305854327343819\n",
      "R^2 on test data: 0.5583717076102472\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 2485 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 45 transformed features from 10 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 5793 feature combinations from 1485 original feature tuples - done.\n",
      "[feateng] Generated altogether 5950 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 925 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 29 features after 5 feature selection runs\n",
      "[featsel] 29 features after correlation filtering\n",
      "[featsel] 8 features after noise filtering\n",
      "[AutoFeat] Computing 8 new features.\n",
      "[AutoFeat]     8/    8 new features ...done.\n",
      "[AutoFeat] Final dataframe with 18 feature columns (8 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-431.1851115851938\n",
      "807772.889550 * x000**3*x001\n",
      "46329.694683 * x009**2*Abs(x000)\n",
      "-3659.915170 * x006*Abs(x009)\n",
      "300.992526 * exp(x002)*exp(x003)\n",
      "275.974942 * exp(x002)*exp(x008)\n",
      "91.379813 * x000**3/x009\n",
      "41.902575 * x004**3/x003\n",
      "14.656672 * Abs(x008)/x008\n",
      "[AutoFeat] Final score: 0.5909\n",
      "[AutoFeat] Computing 8 new features.\n",
      "[AutoFeat]     8/    8 new features ...done.\n",
      "autofeat new features: 8\n",
      "autofeat MSE on training data: 2513.142623733341\n",
      "autofeat MSE on test data: 3271.355839144398\n",
      "autofeat R^2 on training data: 0.5908773752182067\n",
      "autofeat R^2 on test data: 0.3530019689317322\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -2899.1415012022644\n",
      "MSE on training data: 2621.753397592111\n",
      "MSE on test data: 3054.642252269822\n",
      "R^2 on training data: 0.5731962756812963\n",
      "R^2 on test data: 0.39586287153848265\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -3069.4291816180817\n",
      "MSE on training data: 2154.5275450082195\n",
      "MSE on test data: 3559.5622130416536\n",
      "R^2 on training data: 0.6492574850093488\n",
      "R^2 on test data: 0.2960014573329637\n",
      "#### boston\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 4186 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 10381 feature combinations from 2628 original feature tuples - done.\n",
      "[feateng] Generated altogether 10528 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 1190 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 37 features after 5 feature selection runs\n",
      "[featsel] 28 features after correlation filtering\n",
      "[featsel] 15 features after noise filtering\n",
      "[AutoFeat] Computing 15 new features.\n",
      "[AutoFeat]    15/   15 new features ...done.\n",
      "[AutoFeat] Final dataframe with 28 feature columns (15 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "6.859623393950775\n",
      "-20.791136 * log(x004)/x007\n",
      "17.488478 * 1/(x002*x007)\n",
      "13.913569 * sqrt(x008)/x012\n",
      "11.764056 * 1/(x007*x012)\n",
      "2.603826 * x005**3/x009\n",
      "2.507188 * x005**2/x010\n",
      "1.332514 * x004**2*sqrt(x007)\n",
      "0.450821 * exp(x005)/x009\n",
      "-0.040520 * sqrt(x000)*x012\n",
      "0.034567 * x000**3*x003\n",
      "-0.001055 * x010**2*sqrt(x012)\n",
      "-0.000605 * x005**3*x012\n",
      "-0.000340 * exp(x005)*log(x000)\n",
      "0.000015 * x005**3*x011\n",
      "[AutoFeat] Final score: 0.8927\n",
      "[AutoFeat] Computing 15 new features.\n",
      "[AutoFeat]    15/   15 new features ...done.\n",
      "autofeat new features: 15\n",
      "autofeat MSE on training data: 9.123159646825126\n",
      "autofeat MSE on test data: 17.047024579680166\n",
      "autofeat R^2 on training data: 0.8926754440858755\n",
      "autofeat R^2 on test data: 0.7913723801546592\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -11.330046580294077\n",
      "MSE on training data: 7.93496903972113\n",
      "MSE on test data: 16.358112704539764\n",
      "R^2 on training data: 0.9066532800753124\n",
      "R^2 on test data: 0.7998035315337129\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.590279246049377\n",
      "MSE on training data: 1.5032135420792068\n",
      "MSE on test data: 9.889819058823525\n",
      "R^2 on training data: 0.9823162443612512\n",
      "R^2 on test data: 0.8789648362798247\n",
      "#### concrete\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 1596 features.\n",
      "[AutoFeat] With 824 data points this new feature matrix would use about 0.01 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 3272 feature combinations from 861 original feature tuples - done.\n",
      "[feateng] Generated altogether 3456 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 349 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 120 features after 5 feature selection runs\n",
      "[featsel] 69 features after correlation filtering\n",
      "[featsel] 43 features after noise filtering\n",
      "[AutoFeat] Computing 40 new features.\n",
      "[AutoFeat]    40/   40 new features ...done.\n",
      "[AutoFeat] Final dataframe with 48 feature columns (40 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-360.75091858738534\n",
      "-1059.377466 * sqrt(x004)/x000\n",
      "677.684701 * 1/(x000*x007)\n",
      "-376.871897 * log(x007)/x000\n",
      "248.318484 * x003/x005\n",
      "47.260659 * sqrt(x001)/x000\n",
      "-19.009759 * x005/x006\n",
      "8.922117 * log(x003)*log(x006)\n",
      "0.468735 * x004\n",
      "0.354571 * sqrt(x004)*log(x007)\n",
      "0.080647 * sqrt(x002)*sqrt(x007)\n",
      "-0.076874 * x000/x007\n",
      "0.072637 * x000\n",
      "0.065447 * x007**2/x000\n",
      "0.057798 * x002\n",
      "0.022788 * x001*log(x007)\n",
      "0.015853 * x006*log(x007)\n",
      "0.013536 * x001*sqrt(x004)\n",
      "-0.001654 * sqrt(x001)*x004**2\n",
      "-0.001562 * x004**2*sqrt(x007)\n",
      "-0.000163 * sqrt(x002)*x004**3\n",
      "[AutoFeat] Final score: 0.9132\n",
      "[AutoFeat] Computing 40 new features.\n",
      "[AutoFeat]    40/   40 new features ...done.\n",
      "autofeat new features: 40\n",
      "autofeat MSE on training data: 24.748413710843668\n",
      "autofeat MSE on test data: 33.46521377891076\n",
      "autofeat R^2 on training data: 0.9131783881220523\n",
      "autofeat R^2 on test data: 0.8681231970096182\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -28.981941460040474\n",
      "MSE on training data: 24.465226427344973\n",
      "MSE on test data: 33.65666834915904\n",
      "R^2 on training data: 0.9141718569036866\n",
      "R^2 on test data: 0.8673687294957081\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -25.414540562014057\n",
      "MSE on training data: 4.036765567630367\n",
      "MSE on test data: 24.984838842570355\n",
      "R^2 on training data: 0.9858383451379955\n",
      "R^2 on test data: 0.901541920767155\n",
      "#### airfoil\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 630 features.\n",
      "[AutoFeat] With 1202 data points this new feature matrix would use about 0.00 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 484 feature combinations from 325 original feature tuples - done.\n",
      "[feateng] Generated altogether 530 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 201 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 130 features after 5 feature selection runs\n",
      "[featsel] 59 features after correlation filtering\n",
      "[featsel] 45 features after noise filtering\n",
      "[AutoFeat] Computing 41 new features.\n",
      "[AutoFeat]    41/   41 new features ...done.\n",
      "[AutoFeat] Final dataframe with 47 feature columns (42 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "185.55020101090565\n",
      "-53080.637721 * x002**3/x000\n",
      "-45761.736451 * 1/(x000*x003)\n",
      "10426.352270 * x002/x000\n",
      "-6656.579959 * x004/x000\n",
      "-3980.177758 * x002*x004\n",
      "1108.997024 * x002/x003\n",
      "-1070.619190 * sqrt(x004)/x003\n",
      "435.305316 * x004\n",
      "-157.179235 * 1/(x000*x002)\n",
      "-119.244913 * x002\n",
      "64.634588 * x001/x000\n",
      "27.645977 * x003/x000\n",
      "22.100156 * sqrt(x001)*x002**3\n",
      "-16.296350 * 1/(x000*x004)\n",
      "15.344922 * 1/(x002*x003)\n",
      "11.012043 * x001**2*x002**3\n",
      "-10.991819 * sqrt(x000)/x003\n",
      "-5.249177 * x001**3*x004**3\n",
      "4.167449 * x003**2*x004**3\n",
      "-1.978501 * x001\n",
      "1.298921 * x002**3/x004\n",
      "1.258190 * log(x000)*log(x002)\n",
      "-0.511682 * sqrt(x004)/x002\n",
      "0.310442 * 1/(x003*x004)\n",
      "-0.181031 * x002/x004\n",
      "-0.152698 * x003\n",
      "0.036774 * x001**3/x003\n",
      "0.007569 * x000*x004\n",
      "0.004998 * sqrt(x000)/x002\n",
      "-0.004838 * x003**3/x000\n",
      "0.003222 * sqrt(x002)*x003**2\n",
      "-0.000221 * 1/(x002*x004)\n",
      "0.000082 * x003/x004\n",
      "-0.000026 * x001**2/x004\n",
      "0.000021 * sqrt(x000)/x004\n",
      "0.000016 * x001**3/x002\n",
      "[AutoFeat] Final score: 0.8629\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] Computing 41 new features.\n",
      "[AutoFeat]    41/   41 new features ...done.\n",
      "autofeat new features: 41\n",
      "autofeat MSE on training data: 6.430592131822528\n",
      "autofeat MSE on test data: 7.939583771229579\n",
      "autofeat R^2 on training data: 0.8628553566759082\n",
      "autofeat R^2 on test data: 0.8419509656348666\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -7.253124821314216\n",
      "MSE on training data: 6.5037810898569095\n",
      "MSE on test data: 8.061272819370187\n",
      "R^2 on training data: 0.8612944625406362\n",
      "R^2 on test data: 0.8395285670424959\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.5651366037009615\n",
      "MSE on training data: 0.43204418201473005\n",
      "MSE on test data: 2.944272091923948\n",
      "R^2 on training data: 0.990785833710485\n",
      "R^2 on test data: 0.9413899551355542\n",
      "#### wine_quality\n",
      "[AutoFeat] The 2 step feature engineering process could generate up to 3570 features.\n",
      "[AutoFeat] With 5197 data points this new feature matrix would use about 0.07 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 59 transformed features from 12 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 9876 feature combinations from 2485 original feature tuples - done.\n",
      "[feateng] Generated altogether 9959 new features in 2 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 1052 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 118 features after 5 feature selection runs\n",
      "[featsel] 104 features after correlation filtering\n",
      "[featsel] 80 features after noise filtering\n",
      "[AutoFeat] Computing 80 new features.\n",
      "[AutoFeat]    80/   80 new features ...done.\n",
      "[AutoFeat] Final dataframe with 92 feature columns (80 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-16343.950253798354\n",
      "6013.961808 * exp(x007)/x007\n",
      "-104.988198 * x002*log(x007)\n",
      "12.043183 * log(x009)/x006\n",
      "-7.630319 * x004*log(x009)\n",
      "-6.527342 * x002**3/x005\n",
      "4.738257 * log(x000)/x010\n",
      "-4.033020 * sqrt(x001)/x010\n",
      "3.752190 * x004*x011\n",
      "2.016412 * log(x005)/x006\n",
      "1.782823 * x011/x005\n",
      "1.690185 * x001**3/x005\n",
      "0.728011 * log(x003)/x003\n",
      "0.669729 * x001**2*x011\n",
      "-0.653026 * x001**3*log(x003)\n",
      "0.611240 * x009**2/x003\n",
      "0.529456 * x001**3*log(x009)\n",
      "-0.452767 * exp(x008)/x010\n",
      "0.396810 * x004/x001\n",
      "0.255480 * x002**5\n",
      "-0.239974 * x001**3*log(x005)\n",
      "-0.179903 * sqrt(x006)/x005\n",
      "0.169021 * x001**2*x003\n",
      "0.141615 * 1/(x001*x003)\n",
      "-0.123414 * 1/(x000*x001)\n",
      "-0.122716 * exp(x008)/x000\n",
      "-0.114043 * x006*log(x007)\n",
      "0.083850 * x009**5\n",
      "-0.068709 * sqrt(x004)*sqrt(x006)\n",
      "-0.058349 * 1/(x004*x005)\n",
      "-0.048723 * sqrt(x004)*exp(x008)\n",
      "-0.042813 * x001**3/x004\n",
      "0.041915 * x009**2/x001\n",
      "0.031761 * exp(x008)*log(x009)\n",
      "-0.027309 * x002**2/x004\n",
      "-0.025509 * x009/x004\n",
      "0.020566 * x002*x003\n",
      "0.018379 * x003/x001\n",
      "0.016192 * x001**3*x006\n",
      "-0.013566 * x008**3*log(x004)\n",
      "0.009099 * x000**2/x005\n",
      "0.007706 * x000**2*x011\n",
      "-0.006661 * x002**3*x006\n",
      "-0.006463 * x009**3/x004\n",
      "0.006404 * x008**3*log(x005)\n",
      "-0.006140 * x006*x011\n",
      "-0.005674 * x000**2*x009**3\n",
      "-0.005577 * x000**2*sqrt(x002)\n",
      "0.005112 * x000/x001\n",
      "-0.005042 * x000**3*x004\n",
      "0.002827 * x010**3/x006\n",
      "0.002342 * exp(x008)*log(x006)\n",
      "-0.001474 * x000**2*log(x001)\n",
      "0.001274 * x000**3*x002**3\n",
      "0.000954 * x000**2*log(x003)\n",
      "0.000762 * exp(x008)/x004\n",
      "0.000595 * x000**3*log(x009)\n",
      "-0.000354 * x010**3/x003\n",
      "-0.000341 * x005/x001\n",
      "0.000240 * x009**3*x010**3\n",
      "-0.000172 * x005/x004\n",
      "0.000155 * sqrt(x002)*x010**3\n",
      "0.000131 * x005**2*log(x009)\n",
      "-0.000119 * x005**2*x011\n",
      "0.000095 * x010**3/x009\n",
      "-0.000092 * x010**3/x001\n",
      "-0.000062 * x003**3\n",
      "-0.000025 * x006**2*x009**2\n",
      "[AutoFeat] Final score: 0.3966\n",
      "[AutoFeat] Computing 80 new features.\n",
      "[AutoFeat]    80/   80 new features ...done.\n",
      "autofeat new features: 80\n",
      "autofeat MSE on training data: 0.45610763121656495\n",
      "autofeat MSE on test data: 0.48493305337725096\n",
      "autofeat R^2 on training data: 0.39664096085431744\n",
      "autofeat R^2 on test data: 0.3843160842855128\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -0.4794583024730148\n",
      "MSE on training data: 0.4546583603843838\n",
      "MSE on test data: 0.5048993478070777\n",
      "R^2 on training data: 0.39855811943031916\n",
      "R^2 on test data: 0.35896634528287796\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.38427876682460954\n",
      "MSE on training data: 0.050975524340965944\n",
      "MSE on test data: 0.34528061538461535\n",
      "R^2 on training data: 0.9325673562964154\n",
      "R^2 on test data: 0.5616225377507327\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### diabetes\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 60445 features.\n",
      "[AutoFeat] With 353 data points this new feature matrix would use about 0.09 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 45 transformed features from 10 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 5793 feature combinations from 1485 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 24336 transformed features from 5793 original features - done.\n",
      "[feateng] Generated altogether 32161 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 14810 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 29 features after 5 feature selection runs\n",
      "[featsel] 25 features after correlation filtering\n",
      "[featsel] 16 features after noise filtering\n",
      "[AutoFeat] Computing 16 new features.\n",
      "[AutoFeat]    16/   16 new features ...done.\n",
      "[AutoFeat] Final dataframe with 26 feature columns (16 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-234.40825798918885\n",
      "826132.024371 * x000**3*x001\n",
      "-2236.579633 * exp(x006)*Abs(x001)\n",
      "266.831021 * exp(x002)*exp(x003)\n",
      "205.482511 * Abs(x002 + Abs(x009))\n",
      "-138.214584 * Abs(x006 + Abs(x007))\n",
      "115.455141 * exp(x002)*exp(x008)\n",
      "113.634188 * Abs(x006 - Abs(x009))\n",
      "94.081610 * exp(x003)*exp(x008)\n",
      "54.781563 * Abs(x006 - Abs(x003))\n",
      "17.904377 * Abs(x008 + Abs(x009))\n",
      "9.019191 * Abs(x008)/x008\n",
      "1.234075 * x008/Abs(x002)\n",
      "0.046997 * 1/(x002 + x003)\n",
      "0.016564 * 1/(x002 + Abs(x004))\n",
      "0.000115 * x000**3/x009**3\n",
      "-0.000086 * 1/(-x000**3 + x007**2)\n",
      "[AutoFeat] Final score: 0.6379\n",
      "[AutoFeat] Computing 16 new features.\n",
      "[AutoFeat]    16/   16 new features ...done.\n",
      "autofeat new features: 16\n",
      "autofeat MSE on training data: 2224.0192352055146\n",
      "autofeat MSE on test data: 67749.50963454177\n",
      "autofeat R^2 on training data: 0.6379447077616309\n",
      "autofeat R^2 on test data: -12.399275864423728\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -2669.7111417978535\n",
      "MSE on training data: 2321.7258257512044\n",
      "MSE on test data: 92347.49468911672\n",
      "R^2 on training data: 0.6220387355318693\n",
      "R^2 on test data: -17.26418469155979\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.05}\n",
      "best score: -2956.7287625830713\n",
      "MSE on training data: 2019.520143057743\n",
      "MSE on test data: 3681.1720906332675\n",
      "R^2 on training data: 0.6712357771004267\n",
      "R^2 on test data: 0.27194985450249753\n",
      "#### boston\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 102466 features.\n",
      "[AutoFeat] With 404 data points this new feature matrix would use about 0.17 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 60 transformed features from 13 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 10381 feature combinations from 2628 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 42017 transformed features from 10381 original features - done.\n",
      "[feateng] Generated altogether 54631 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 24242 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 51 features after 5 feature selection runs\n",
      "[featsel] 29 features after correlation filtering\n",
      "[featsel] 21 features after noise filtering\n",
      "[AutoFeat] Computing 21 new features.\n",
      "[AutoFeat]    21/   21 new features ...done.\n",
      "[AutoFeat] Final dataframe with 34 feature columns (21 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "17.72942048302577\n",
      "28.815898 * 1/(sqrt(x012) + log(x007))\n",
      "13.518786 * 1/(x012*log(x007))\n",
      "-4.457275 * 1/(sqrt(x006) - x012**2)\n",
      "-2.903010 * x004**6*log(x000)**2\n",
      "-1.041790 * log(x000**2 + x010**2)\n",
      "0.705949 * Abs(x005 - log(x009))\n",
      "-0.390242 * 1/(-x002**2 + log(x011))\n",
      "0.336811 * exp(x005**3/x009)\n",
      "0.327107 * (-log(x012) + 1/x004)**3\n",
      "-0.094387 * 1/(log(x000) - 1/x012)\n",
      "0.083302 * 1/(-log(x007) + 1/x005)\n",
      "0.079690 * x005**6/x010**3\n",
      "-0.032739 * 1/(sqrt(x008) - log(x006))\n",
      "0.012054 * (-sqrt(x009) + x012)**2\n",
      "0.001243 * exp(-x000 + x005)\n",
      "-0.000919 * exp(sqrt(x006) - x008)\n",
      "-0.000697 * sqrt(x009**3/x008)\n",
      "-0.000607 * exp(x005)*log(x000)\n",
      "0.000021 * x005**3*x011\n",
      "[AutoFeat] Final score: 0.9322\n",
      "[AutoFeat] Computing 21 new features.\n",
      "[AutoFeat]    21/   21 new features ...done.\n",
      "autofeat new features: 21\n",
      "autofeat MSE on training data: 5.76749323048052\n",
      "autofeat MSE on test data: 77.76919148430842\n",
      "autofeat R^2 on training data: 0.9321513956061865\n",
      "autofeat R^2 on test data: 0.04823265545075994\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 0.1}\n",
      "best score: -13.170130608152538\n",
      "MSE on training data: 5.1938204307102485\n",
      "MSE on test data: 105.84008986660551\n",
      "R^2 on training data: 0.9389000639249289\n",
      "R^2 on test data: -0.2953090980702493\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -10.53117710123456\n",
      "MSE on training data: 1.3112844950495042\n",
      "MSE on test data: 11.858764921568623\n",
      "R^2 on training data: 0.984574091481865\n",
      "R^2 on test data: 0.8548681684402957\n",
      "#### concrete\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 38556 features.\n",
      "[AutoFeat] With 824 data points this new feature matrix would use about 0.13 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 34 transformed features from 8 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 3272 feature combinations from 861 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 9450 transformed features from 3272 original features - done.\n",
      "[feateng] Generated altogether 14485 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 4421 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 32 features after 5 feature selection runs\n",
      "[featsel] 19 features after correlation filtering\n",
      "[featsel] 16 features after noise filtering\n",
      "[AutoFeat] Computing 16 new features.\n",
      "[AutoFeat]    16/   16 new features ...done.\n",
      "[AutoFeat] Final dataframe with 24 feature columns (16 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "-47.884575425009196\n",
      "-1947.981371 * 1/(x000 + x001)\n",
      "348.538617 * log(x007)/x003\n",
      "192.185965 * sqrt(x000)/x003\n",
      "-29.798026 * 1/(sqrt(x003) - x004**3)\n",
      "-15.993150 * 1/(sqrt(x002) + x007)\n",
      "3.344331 * log(x000**3*x007)\n",
      "-1.259628 * 1/(-sqrt(x007) + log(x005))\n",
      "0.504765 * sqrt(sqrt(x004)*x007)\n",
      "0.159978 * sqrt(x001)*log(x007)\n",
      "-0.011752 * Abs(x003 - x004**2)\n",
      "-0.000365 * sqrt(x000**3*sqrt(x004))\n",
      "-0.000088 * sqrt(x000**3*x001)\n",
      "-0.000059 * (-x003 + x007)**2\n",
      "[AutoFeat] Final score: 0.8667\n",
      "[AutoFeat] Computing 16 new features.\n",
      "[AutoFeat]    16/   16 new features ...done.\n",
      "autofeat new features: 16\n",
      "autofeat MSE on training data: 38.00909183806815\n",
      "autofeat MSE on test data: 44.62030363728246\n",
      "autofeat R^2 on training data: 0.8666576913593416\n",
      "autofeat R^2 on test data: 0.8241641893872148\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -39.81712172150789\n",
      "MSE on training data: 36.54360918007186\n",
      "MSE on test data: 44.72917134406798\n",
      "R^2 on training data: 0.8717988518406966\n",
      "R^2 on test data: 0.8237351729997918\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -26.046865105046898\n",
      "MSE on training data: 4.085895566529815\n",
      "MSE on test data: 27.584884050496488\n",
      "R^2 on training data: 0.9856659888106022\n",
      "R^2 on test data: 0.8912958888153781\n",
      "#### airfoil\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 14910 features.\n",
      "[AutoFeat] With 1202 data points this new feature matrix would use about 0.07 gb of space.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 21 transformed features from 5 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 484 feature combinations from 325 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 1610 transformed features from 484 original features - done.\n",
      "[feateng] Generated altogether 2355 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 1057 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 113 features after 5 feature selection runs\n",
      "[featsel] 59 features after correlation filtering\n",
      "[featsel] 44 features after noise filtering\n",
      "[AutoFeat] Computing 43 new features.\n",
      "[AutoFeat]    43/   43 new features ...done.\n",
      "[AutoFeat] Final dataframe with 49 feature columns (44 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "138.29052097779504\n",
      "-49056.876531 * x001*x004**4\n",
      "33796.579252 * 1/(sqrt(x000)*x003**2)\n",
      "18163.842249 * x000*exp(-sqrt(x000))\n",
      "-2163.953781 * sqrt(x004)/x003\n",
      "2022.489662 * x002*exp(-1/x002)\n",
      "-1590.946499 * 1/(x000**2*x004)\n",
      "-1173.062259 * sqrt(x002**3/x000)\n",
      "976.191964 * log(x002)/x000\n",
      "879.089128 * 1/(sqrt(x000) + 1/x002)\n",
      "712.121116 * 1/(sqrt(x002)*x003**3)\n",
      "-674.724814 * sqrt(x002)/x003\n",
      "306.646820 * 1/(x000**2*x002**2)\n",
      "198.621522 * x001**2*x002**6\n",
      "-34.298021 * x001*x002**2\n",
      "-27.208080 * 1/(x000*x004)\n",
      "25.450439 * x001**9*x004**9\n",
      "9.053086 * 1/(x000**2*x004**2)\n",
      "1.789545 * 1/(x002**3*x003**3)\n",
      "-1.574230 * x001**4/x000**2\n",
      "-1.261320 * x002**2/x004\n",
      "-1.139647 * x003/log(x000)\n",
      "-0.733166 * 1/(exp(x002) + log(x002))\n",
      "0.175708 * x002/x004\n",
      "0.054867 * 1/(x003*x004)\n",
      "-0.033696 * sqrt(x000*x001)\n",
      "0.010609 * 1/(x002**2 + 1/x000)\n",
      "-0.002860 * x000\n",
      "-0.002610 * 1/(x000*x004**2)\n",
      "-0.001756 * x001**9*x004**6\n",
      "0.000190 * x001**(1/4)*Abs(x000)\n",
      "0.000183 * x001**6/x003**3\n",
      "0.000098 * x001**2/x004\n",
      "0.000083 * x000**2*x004**2\n",
      "-0.000031 * x001**3/x004\n",
      "0.000019 * (sqrt(x000) - 1/x002)**3\n",
      "0.000011 * sqrt(x002)*x003**3\n",
      "[AutoFeat] Final score: 0.8844\n",
      "[AutoFeat] Applying the Pi Theorem\n",
      "[AutoFeat] Pi Theorem 1:  x001 * x003 / x004\n",
      "[AutoFeat] Computing 43 new features.\n",
      "[AutoFeat]    43/   43 new features ...done.\n",
      "autofeat new features: 43\n",
      "autofeat MSE on training data: 5.420907947435369\n",
      "autofeat MSE on test data: 5.884339748903656\n",
      "autofeat R^2 on training data: 0.8843887978426251\n",
      "autofeat R^2 on test data: 0.8828636057017676\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1e-05}\n",
      "best score: -6.133419806069622\n",
      "MSE on training data: 5.51294657216502\n",
      "MSE on test data: 5.904354544199249\n",
      "R^2 on training data: 0.8824258986100454\n",
      "R^2 on test data: 0.8824651819102148\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -3.5912635475720833\n",
      "MSE on training data: 0.4544312614041602\n",
      "MSE on test data: 3.000684251330904\n",
      "R^2 on training data: 0.9903083865399923\n",
      "R^2 on test data: 0.9402669885446568\n",
      "#### wine_quality\n",
      "[AutoFeat] The 3 step feature engineering process could generate up to 87234 features.\n",
      "[AutoFeat] With 5197 data points this new feature matrix would use about 1.81 gb of space.\n",
      "[feateng] Step 1: transformation of original features\n",
      "[feateng] Generated 59 transformed features from 12 original features - done.\n",
      "[feateng] Step 2: first combination of features\n",
      "[feateng] Generated 9876 feature combinations from 2485 original feature tuples - done.\n",
      "[feateng] Step 3: transformation of new features\n",
      "[feateng] Generated 43976 transformed features from 9876 original features - done.\n",
      "[feateng] Generated altogether 55648 new features in 3 steps\n",
      "[feateng] Removing correlated features, as well as additions at the highest level\n",
      "[feateng] Generated a total of 23287 additional features\n",
      "[featsel] Scaling data...done.\n",
      "[featsel] Feature selection run 1/5\n",
      "[featsel] Feature selection run 2/5\n",
      "[featsel] Feature selection run 3/5\n",
      "[featsel] Feature selection run 4/5\n",
      "[featsel] Feature selection run 5/5\n",
      "[featsel] 68 features after 5 feature selection runs\n",
      "[featsel] 49 features after correlation filtering\n",
      "[featsel] 26 features after noise filtering\n",
      "[AutoFeat] Computing 26 new features.\n",
      "[AutoFeat]    26/   26 new features ...done.\n",
      "[AutoFeat] Final dataframe with 38 feature columns (26 new).\n",
      "[AutoFeat] Training final regression model.\n",
      "[AutoFeat] Trained model: largest coefficients:\n",
      "5.394206193634702\n",
      "-701.854725 * exp(-x000**2 + x008**2)\n",
      "-152.223850 * 1/(x003**3 + x010**3)\n",
      "12.619869 * x009**2/x006\n",
      "1.855807 * exp(2*x008)*log(x007)**2\n",
      "1.625017 * 1/(x001**2 - sqrt(x010))\n",
      "0.726473 * Abs(1/x005 - 1/x000)\n",
      "0.519617 * exp(-x001)/log(x006)\n",
      "-0.179612 * (x002 - 1/x003)**2\n",
      "-0.164964 * x001**2*log(x009)**2\n",
      "-0.149505 * Abs(x009**2 - 1/x003)\n",
      "-0.105722 * 1/(-x001**3 + exp(x009))\n",
      "0.081542 * Abs(sqrt(x000) + log(x004))\n",
      "-0.079287 * (log(x004) + log(x005))**2\n",
      "-0.024543 * 1/(x002 + x009**3)\n",
      "0.011513 * 1/(sqrt(x001) - x003**2)\n",
      "0.005686 * (log(x005) - log(x006))**3\n",
      "0.004065 * 1/(x001**3 - log(x007))\n",
      "0.002772 * exp(x011)/x004\n",
      "0.001128 * sqrt(x009)*x010**2\n",
      "0.000174 * x010**3*log(x005)\n",
      "-0.000090 * exp(-x003**2 + sqrt(x005))\n",
      "0.000075 * sqrt(x002)*x010**3\n",
      "0.000012 * sqrt(x003)*x010**3\n",
      "[AutoFeat] Final score: 0.3537\n",
      "[AutoFeat] Computing 26 new features.\n",
      "[AutoFeat]    26/   26 new features ...done.\n",
      "autofeat new features: 26\n",
      "autofeat MSE on training data: 0.4885774469892906\n",
      "autofeat MSE on test data: 181.17109138186638\n",
      "autofeat R^2 on training data: 0.3536884744124339\n",
      "autofeat R^2 on test data: -229.01964122556842\n",
      "# Ridge Regression\n",
      "best params: {'alpha': 1.0}\n",
      "best score: -0.5045331769830724\n",
      "MSE on training data: 0.4916945720693446\n",
      "MSE on test data: 0.518018702556771\n",
      "R^2 on training data: 0.3495650055983258\n",
      "R^2 on test data: 0.3423096631951448\n",
      "# Random Forest\n",
      "best params: {'min_samples_leaf': 0.0001}\n",
      "best score: -0.39452466104612427\n",
      "MSE on training data: 0.05216649990379064\n",
      "MSE on test data: 0.3473213846153846\n",
      "R^2 on training data: 0.9309918819520919\n",
      "R^2 on test data: 0.5590315228006912\n"
     ]
    }
   ],
   "source": [
    "for dsname in datasets:\n",
    "    print(\"####\", dsname)\n",
    "    test_autofeat(dsname, feateng_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
